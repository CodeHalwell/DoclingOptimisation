{"success":true,"data":{"web":[{"url":"https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html","title":"Graph Optimizations in ONNX Runtime","description":"They are run after graph partitioning and are only applied to nodes assigned to CPU execution provider. Available layout optimizations are as follows:","position":1},{"url":"https://iot-robotics.github.io/ONNXRuntime/docs/performance/tune-performance.html","title":"ONNX Runtime Performance Tuning - GitHub Pages","description":"ONNX Runtime provides high performance across a range of hardware options through its Execution Providers interface for different execution environments.","position":2},{"url":"https://onnxruntime.ai/docs/performance/tune-performance/threading.html","title":"Thread management","description":"You can customize the performance using the following knobs in the API to control the thread count and other settings.","position":3},{"url":"https://onnxruntime.ai/docs/api/python/api_summary.html","title":"Python API documentation","description":"ONNX Runtime orchestrates the execution of operator kernels via execution providers . An execution provider contains the set of kernels for a specific execution ...","position":4},{"url":"https://medium.com/@Modexa/8-onnx-runtime-tricks-for-low-latency-python-inference-baee6e535445","title":"8 ONNX Runtime Tricks for Low-Latency Python Inference - Medium","description":"ONNX Runtime (ORT) will try providers in the order you pass them. Put your fastest first and avoid silent fallbacks. import onnxruntime as ort","position":5}]}}